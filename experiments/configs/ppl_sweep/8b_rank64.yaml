# PPL sweep: 8B with LoRA rank 64
model:
  backend: "torch"
  path: "meta-llama/Llama-3.1-8B-Instruct"
  max_tokens: 512
  temperature: 0.7
  top_p: 0.9
  repetition_penalty: 1.1

context:
  max_tokens: 4096
  compaction_threshold: 0.8
  system_prompt: "You may recall things from previous conversations."

sleep:
  light_sleep_turns: 999
  deep_sleep_interval: 5
  manual_trigger: "/sleep"
  curation:
    min_novelty_score: 0.0
    min_importance_score: 0.0
    min_combined_score: 0.0
  firewall:
    min_grounding_score: 0.3
    use_model_verification: false

lora:
  rank: 64
  alpha: 128
  layers: 8
  light_learning_rate: 1.0e-4
  deep_learning_rate: 5.0e-5
  light_epochs: 1
  deep_epochs: 1
  batch_size: 1

replay:
  max_items: 1000
  light_mix_ratio: 0.2
  deep_mix_ratio: 0.6
  decay_factor: 0.85
  min_priority: 0.05

validation:
  min_score_ratio: 0.5
  num_questions: 5

dreamer:
  num_dreams: 5
  temperature: 0.9

memit:
  enabled: true
  target_layers: [8, 9, 10, 11, 12, 13, 14, 15]
  lambda_reg: 0.1
  max_active_edits: 50
  covariance_samples: 200

paths:
  conversations: "data/conversations"
  training: "data/training"
  replay_buffer: "data/replay_buffer"
  core_identity: "data/core_identity"
  benchmarks: "data/benchmarks"
  base_model: "models/base"
  current_model: "models/current"
  checkpoints: "models/checkpoints"
  adapters: "adapters"
  memit_ledger: "data/memit/ledger.json"
  memit_data: "data/memit"
