# 70B V7 Comprehensive Experiment — MEMIT-only (no LoRA)
# Llama-3.1-70B has 80 layers; target 24 mid-to-upper layers [28-51]
# - Layers [36-43] got 0.80 recall at 40 facts in prior experiments
# - 24 layers centered on the sweet spot gives 3x capacity slots
# - VRAM: 24 × ~450MB dequantized down_proj ≈ 10.8GB + 38GB model ≈ 49GB (fits 2×H100)
model:
  backend: "torch"
  path: "meta-llama/Llama-3.1-70B-Instruct"
  quantize: "4bit"
  max_tokens: 512
  temperature: 0.7
  top_p: 0.9
  repetition_penalty: 1.1

context:
  max_tokens: 4096
  compaction_threshold: 0.8
  system_prompt: "You may recall things from previous conversations."

sleep:
  light_sleep_turns: 100
  deep_sleep_interval: 5
  manual_trigger: "/sleep"
  trigger_mode: "health"
  curation:
    min_novelty_score: 0.0
    min_importance_score: 0.0
    min_combined_score: 0.0
  firewall:
    min_grounding_score: 0.3
    use_model_verification: false
  maintenance:
    degraded_threshold: 0.5
    max_ppl_increase: 0.15
    max_refresh_per_cycle: 20
    nap_audit_count: 10

memit:
  target_layers: [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
  lambda_reg: 0.1
  max_active_edits: 120
  enabled: true
  target_module: "down_proj"
  covariance_samples: 100
  residual_scale: 2.0
  v_lr: 0.5
  v_steps: 30
  v_kl_factor: 0.0625

health:
  nap_threshold: 0.4
  sleep_threshold: 0.8
  edit_weight: 0.6
  time_weight: 0.3
  perplexity_weight: 0.1
  perplexity_check_interval: 0
  max_wake_seconds: 7200

validation:
  min_score_ratio: 0.5
  num_questions: 5

paths:
  conversations: "data/conversations"
  training: "data/training"
  replay_buffer: "data/replay_buffer"
  core_identity: "data/core_identity"
  benchmarks: "data/benchmarks"
  base_model: "models/base"
  current_model: "models/current"
  checkpoints: "models/checkpoints"
  adapters: "adapters"
  memit_data: "data/memit"
  memit_ledger: "data/memit/ledger.json"
