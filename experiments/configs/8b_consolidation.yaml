# 8B Consolidation Test — torch backend for H100
# Tests MEMIT→LoRA knowledge transfer during sleep
# Llama-3.1-8B: 32 layers, target [12-19], LoRA on last 8 layers
model:
  backend: "torch"
  path: "meta-llama/Llama-3.1-8B-Instruct"
  quantize: "4bit"
  max_tokens: 512
  temperature: 0.7
  top_p: 0.9
  repetition_penalty: 1.1

context:
  max_tokens: 4096
  compaction_threshold: 0.8
  system_prompt: "You may recall things from previous conversations."

sleep:
  manual_trigger: "/sleep"
  trigger_mode: "health"
  curation:
    min_novelty_score: 0.0
    min_importance_score: 0.0
    min_combined_score: 0.0
  firewall:
    min_grounding_score: 0.3
    use_model_verification: false
  maintenance:
    degraded_threshold: 0.5
    max_ppl_increase: 0.15
    max_refresh_per_cycle: 10
    nap_audit_count: 5

memit:
  target_layers: [12, 13, 14, 15, 16, 17, 18, 19]
  lambda_reg: 0.1
  max_active_edits: 50
  enabled: true
  target_module: "down_proj"
  covariance_samples: 200
  residual_scale: 2.0
  v_lr: 0.5
  v_steps: 30
  v_kl_factor: 0.0625

lora:
  enabled: true
  num_layers: 8
  learning_rate: 1.0e-4
  iters_per_fact: 10
  batch_size: 1

consolidation:
  enabled: true
  scale_schedule: [1.0, 0.5, 0.1, 0.0]

health:
  nap_threshold: 0.4
  sleep_threshold: 0.8
  edit_weight: 0.6
  time_weight: 0.3
  perplexity_weight: 0.1
  perplexity_check_interval: 0
  max_wake_seconds: 7200

nap:
  manual_trigger: "/nap"

validation:
  min_score_ratio: 0.5
  num_questions: 5

paths:
  conversations: "data/conversations"
  core_identity: "data/core_identity"
  benchmarks: "data/benchmarks"
  memit_data: "data/memit"
  memit_ledger: "data/memit/ledger.json"
  adapters: "data/adapters"
  fused_models: "models/fused"
