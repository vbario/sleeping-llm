% ═══════════════════════════════════════════════════════════════
% References for "Dual-System Memory Consolidation" paper (v2)
% ═══════════════════════════════════════════════════════════════

% ── Knowledge editing: ROME, MEMIT ──

@inproceedings{meng2022rome,
  title={Locating and Editing Factual Associations in {GPT}},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@inproceedings{meng2023memit,
  title={Mass-Editing Memory in a Transformer},
  author={Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex J. and Belinkov, Yonatan and Bau, David},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@article{mitchell2022fast,
  title={Fast Model Editing at Scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Finn, Chelsea and Manning, Christopher D.},
  journal={arXiv preprint arXiv:2110.11309},
  year={2022}
}

@article{yao2023editing,
  title={Editing Large Language Models: Problems, Methods, and Opportunities},
  author={Yao, Yunzhi and Wang, Peng and Tian, Bozhong and Cheng, Siyuan and Li, Zhoubo and Deng, Shumin and Chen, Huajun and Zhang, Ningyu},
  journal={arXiv preprint arXiv:2305.13172},
  year={2023}
}

@article{deeldar2024knowledge,
  title={Knowledge Editing for Large Language Models: A Survey},
  author={De Cao, Nicola and Aziz, Wilker and Titov, Ivan},
  journal={arXiv preprint arXiv:2310.16218},
  year={2024}
}

% ── LoRA and parameter-efficient fine-tuning ──

@inproceedings{hu2022lora,
  title={{LoRA}: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{dettmers2023qlora,
  title={{QLoRA}: Efficient Finetuning of Quantized {LLMs}},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-Efficient Transfer Learning for {NLP}},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and de Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019}
}

@inproceedings{li2021prefix,
  title={Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author={Li, Xiang Lisa and Liang, Percy},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  pages={4582--4597},
  year={2021}
}

% ── CLS theory and neuroscience ──

@article{mcclelland1995why,
  title={Why There Are Complementary Learning Systems in the Hippocampus and Neocortex: Insights from the Successes and Failures of Connectionist Models of Learning and Memory},
  author={McClelland, James L. and McNaughton, Bruce L. and O'Reilly, Randall C.},
  journal={Psychological Review},
  volume={102},
  number={3},
  pages={419--457},
  year={1995}
}

@article{kumaran2016what,
  title={What Learning Systems do Intelligent Agents Need? {Complementary Learning Systems} Theory Updated},
  author={Kumaran, Dharshan and Hassabis, Demis and McClelland, James L.},
  journal={Trends in Cognitive Sciences},
  volume={20},
  number={7},
  pages={512--534},
  year={2016}
}

@article{diekelmann2010memory,
  title={The Memory Function of Sleep},
  author={Diekelmann, Susanne and Born, Jan},
  journal={Nature Reviews Neuroscience},
  volume={11},
  number={2},
  pages={114--126},
  year={2010}
}

@article{rasch2013sleep,
  title={About Sleep's Role in Memory},
  author={Rasch, Bj{\"o}rn and Born, Jan},
  journal={Physiological Reviews},
  volume={93},
  number={2},
  pages={681--766},
  year={2013}
}

% ── Sleep-inspired ML ──

@article{tadros2022sleep,
  title={Sleep-like Unsupervised Replay Reduces Catastrophic Forgetting in Artificial Neural Networks},
  author={Tadros, Timothy and Krishnan, Giri P. and Ramyaa, Ramyaa and Bazhenov, Maxim},
  journal={Nature Communications},
  volume={13},
  pages={7742},
  year={2022}
}

@article{carta2024wake,
  title={Wake-Sleep Consolidated Learning},
  author={Carta, Antonio and others},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  note={Also arXiv:2401.08623}
}

@article{harun2023siesta,
  title={{SIESTA}: Efficient Online Continual Learning with Sleep},
  author={Harun, Md Yousuf and Gallardo, Jhair and Hayes, Tyler L. and Kemker, Ronald and Kanan, Christopher},
  journal={Transactions on Machine Learning Research},
  year={2023}
}

@article{hinton1995wake,
  title={The ``Wake-Sleep'' Algorithm for Unsupervised Neural Networks},
  author={Hinton, Geoffrey E. and Dayan, Peter and Frey, Brendan J. and Neal, Radford M.},
  journal={Science},
  volume={268},
  number={5214},
  pages={1158--1161},
  year={1995}
}

@article{krishnan2019biologically,
  title={Biologically Inspired Sleep Algorithm for Artificial Neural Networks},
  author={Krishnan, Giri P. and Tadros, Timothy and Ramyaa, Ramyaa and Bazhenov, Maxim},
  journal={arXiv preprint arXiv:1908.02240},
  year={2019}
}

@article{lmneedsleep2025,
  title={Language Models Need Sleep: Learning to Self Modify and Consolidate Memories},
  author={Anonymous},
  journal={OpenReview},
  year={2025}
}

@article{dreaming2024,
  title={Dreaming is All You Need},
  author={Anonymous},
  journal={arXiv preprint arXiv:2409.01633},
  year={2024}
}

% ── Continual learning ──

@article{kirkpatrick2017overcoming,
  title={Overcoming Catastrophic Forgetting in Neural Networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
  journal={Proceedings of the National Academy of Sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017}
}

@article{li2017learning,
  title={Learning without Forgetting},
  author={Li, Zhizhong and Hoiem, Derek},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={40},
  number={12},
  pages={2935--2947},
  year={2017}
}

@article{rusu2016progressive,
  title={Progressive Neural Networks},
  author={Rusu, Andrei A. and Rabinowitz, Neil C. and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  journal={arXiv preprint arXiv:1606.04671},
  year={2016}
}

@article{wu2024continual,
  title={Continual Learning for Large Language Models: A Survey},
  author={Wu, Tongtong and Luo, Linhao and Li, Yuan-Fang and Pan, Shirui and Vu, Thuy-Trang and Haffari, Gholamreza},
  journal={arXiv preprint arXiv:2402.01364},
  year={2024}
}

@article{shi2024continual,
  title={Continual Learning of Large Language Models: A Comprehensive Survey},
  author={Shi, Haizhou and Xu, Zihao and Wang, Hengyi and Qin, Weiyi and Wang, Wenyuan and Wang, Yibin and Wang, Hao},
  journal={ACM Computing Surveys},
  year={2024}
}

@article{luo2023empirical,
  title={An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning},
  author={Luo, Yun and Yang, Zhen and Meng, Fandong and Li, Yafu and Zhou, Jie and Zhang, Yue},
  journal={arXiv preprint arXiv:2308.08747},
  year={2023}
}

@inproceedings{li2024revisiting,
  title={Revisiting Catastrophic Forgetting in Large Language Model Tuning},
  author={Li, Hongyu and Ding, Liang and Fang, Meng and Tao, Dacheng},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  year={2024}
}

@inproceedings{zheng2025spurious,
  title={Spurious Forgetting in Continual Learning of Language Models},
  author={Zheng, Jianheng and Cai, Xin and Qiu, Siheng and Ma, Qianli},
  booktitle={International Conference on Learning Representations},
  year={2025}
}

@inproceedings{wang2023trace,
  title={{TRACE}: A Comprehensive Benchmark for Continual Learning in Large Language Models},
  author={Wang, Xiao and Zhang, Yuansen and Chen, Tianze and Gao, Songyang and Jin, Shengjie and Yang, Xinyi and Xi, Zhiheng and Zheng, Rui and Zou, Yicheng and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  year={2024},
  note={arXiv:2310.06762}
}

@inproceedings{wang2023olora,
  title={{O-LoRA}: Orthogonal Subspace Learning for Language Model Continual Learning},
  author={Wang, Xiao and Chen, Tianze and Ge, Qiming and Xia, Han and Bao, Rong and Zheng, Rui and Zhang, Qi and Gui, Tao and Huang, Xuanjing},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={10658--10671},
  year={2023}
}

@inproceedings{liang2024inflora,
  title={{InfLoRA}: Interference-Free Low-Rank Adaptation for Continual Learning},
  author={Liang, Yan-Shuo and Li, Wu-Jun},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={23638--23647},
  year={2024}
}

% ── Experience replay ──

@inproceedings{lopezpaz2017gradient,
  title={Gradient Episodic Memory for Continual Learning},
  author={Lopez-Paz, David and Ranzato, Marc'Aurelio},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{chaudhry2019efficient,
  title={Efficient Lifelong Learning with {A-GEM}},
  author={Chaudhry, Arslan and Ranzato, Marc'Aurelio and Rohrbach, Marcus and Elhoseiny, Mohamed},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{buzzega2020dark,
  title={Dark Experience for General Continual Learning: a Strong, Simple Baseline},
  author={Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Abati, Davide and Calderara, Simone},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15920--15930},
  year={2020}
}

@inproceedings{rolnick2019experience,
  title={Experience Replay for Continual Learning},
  author={Rolnick, David and Ahuja, Arun and Schwarz, Jonathan and Lillicrap, Timothy and Wayne, Gregory},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{huang2024mitigating,
  title={Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal},
  author={Huang, Jianheng and Cui, Leyang and Wang, Ante and Yang, Chengyi and Liao, Xinting and Song, Linfeng and Yao, Junfeng and Su, Jinsong},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics},
  year={2024}
}

% ── RAG and memory-augmented models ──

@inproceedings{lewis2020retrieval,
  title={Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@inproceedings{wang2024memoryllm,
  title={{MemoryLLM}: Towards Self-Updatable Large Language Models},
  author={Wang, Yu and Gao, Yifan and Chen, Xiusi and Jiang, Haoming and Li, Shiyang and Yang, Jingfeng and Yin, Qingyu and Li, Zheng and Li, Xian and Yin, Bing and Shang, Jingbo and McAuley, Julian},
  booktitle={International Conference on Machine Learning},
  year={2024}
}

@inproceedings{wang2023longmem,
  title={Augmenting Language Models with Long-Term Memory},
  author={Wang, Weizhi and Dong, Li and Cheng, Hao and Liu, Xiaodong and Yan, Xifeng and Gao, Jianfeng and Wei, Furu},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@inproceedings{ovadia2024finetuning,
  title={Fine-Tuning or Retrieval? Comparing Knowledge Injection in {LLMs}},
  author={Ovadia, Oded and Brief, Menachem and Mishaeli, Moshik and Elisha, Oren},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  year={2024}
}

@article{deluisbalaguer2024rag,
  title={{RAG} vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture},
  author={{de Luis Balaguer}, Maria Angels and Benara, Vinamra and {de Freitas Cunha}, Renato Luiz and others},
  journal={arXiv preprint arXiv:2401.08406},
  year={2024}
}

% ── Self-training and synthetic data ──

@inproceedings{wang2023selfinstruct,
  title={Self-Instruct: Aligning Language Models with Self-Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics},
  year={2023}
}

@inproceedings{chen2024selfplay,
  title={Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models},
  author={Chen, Zixiang and Deng, Yihe and Yuan, Huizhuo and Ji, Kaixuan and Gu, Quanquan},
  booktitle={International Conference on Machine Learning},
  year={2024}
}

@article{bai2022constitutional,
  title={Constitutional {AI}: Harmlessness from {AI} Feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@inproceedings{lin2024rho1,
  title={Rho-1: Not All Tokens Are What You Need},
  author={Lin, Zhenghao and Gou, Zhibin and Gong, Yeyun and Liu, Xiao and Shen, Yelong and Xu, Ruochen and Lin, Chen and Yang, Yujiu and Jiao, Jian and Duan, Nan and Chen, Weizhu},
  booktitle={Advances in Neural Information Processing Systems},
  year={2024}
}

@inproceedings{cheng2024adapting,
  title={Adapting Large Language Models to Domains via Reading Comprehension},
  author={Cheng, Daixuan and Huang, Shaohan and Wei, Furu},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

% ── MLX framework ──

@misc{hannun2023mlx,
  title={{MLX}: Efficient and Flexible Machine Learning on {Apple} Silicon},
  author={Hannun, Awni and Digani, Jagrit and Katharopoulos, Angelos and Collobert, Ronan},
  year={2023},
  howpublished={Apple Machine Learning Research. \url{https://github.com/ml-explore/mlx}}
}

% ── Matrix algebra for MEMIT ──

@article{woodbury1950inverting,
  title={Inverting Modified Matrices},
  author={Woodbury, Max A.},
  journal={Memorandum Report},
  volume={42},
  pages={336},
  year={1950},
  publisher={Statistical Research Group, Princeton University}
}

% ── Our v1 paper ──

@article{baranov2026sleepwake,
  title={Sleep-Wake Consolidation for Lifelong Conversational Memory in Local Language Models},
  author={Baranov, Vladimir},
  journal={arXiv preprint},
  year={2026},
  note={v1 of this work}
}
