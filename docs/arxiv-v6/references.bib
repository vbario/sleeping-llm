% ═══════════════════════════════════════════════════════════════
% References for "Per-Fact Graduated Consolidation Resolves
% the Capacity Ceiling in Weight-Edited Language Models" (v6)
% ═══════════════════════════════════════════════════════════════

% ── Knowledge editing: ROME, MEMIT ──

@inproceedings{meng2022rome,
  title={Locating and Editing Factual Associations in {GPT}},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@inproceedings{meng2023memit,
  title={Mass-Editing Memory in a Transformer},
  author={Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex J. and Belinkov, Yonatan and Bau, David},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

% ── CLS theory, neuroscience, and sleep ──

@article{mcclelland1995why,
  title={Why There Are Complementary Learning Systems in the Hippocampus and Neocortex: Insights from the Successes and Failures of Connectionist Models of Learning and Memory},
  author={McClelland, James L. and McNaughton, Bruce L. and O'Reilly, Randall C.},
  journal={Psychological Review},
  volume={102},
  number={3},
  pages={419--457},
  year={1995}
}

@article{walker2004sleep,
  title={Sleep-Dependent Learning and Memory Consolidation},
  author={Walker, Matthew P. and Stickgold, Robert},
  journal={Neuron},
  volume={44},
  number={1},
  pages={121--133},
  year={2004}
}

% ── Continual learning ──

@article{kirkpatrick2017overcoming,
  title={Overcoming Catastrophic Forgetting in Neural Networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Blundell, Charles and Wierstra, Daan and Hadsell, Raia},
  journal={Proceedings of the National Academy of Sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017}
}

% ── Parameter-efficient fine-tuning ──

@inproceedings{hu2022lora,
  title={{LoRA}: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{dettmers2023qlora,
  title={{QLoRA}: Efficient Finetuning of Quantized {LLMs}},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

% ── Our prior papers ──

@article{baranov2026sleepwake,
  title={Sleep-Wake Consolidation for Lifelong Conversational Memory in Local Language Models},
  author={Baranov, Vladimir},
  journal={Zenodo},
  year={2026},
  doi={10.5281/zenodo.18778760},
  note={Paper 1 of this series}
}

@article{baranov2026alignmenttax,
  title={The Alignment Tax on Continual Learning: Inverse Scaling of Memory Consolidation in Language Models},
  author={Baranov, Vladimir},
  journal={Zenodo},
  year={2026},
  doi={10.5281/zenodo.18778762},
  note={Paper 2 of this series}
}

@article{baranov2026dualsystem,
  title={Dual-System Memory Consolidation for Lifelong Learning in Language Models: Combining Direct Weight Editing with Sleep-Wake Training},
  author={Baranov, Vladimir},
  journal={Zenodo},
  year={2026},
  doi={10.5281/zenodo.18778764},
  note={Paper 3 of this series}
}

@article{baranov2026twophase,
  title={Sleeping {LLM}: Two-Phase Memory Consolidation for Lifelong Learning from {3B} to {70B} Parameters},
  author={Baranov, Vladimir},
  journal={Zenodo},
  year={2026},
  doi={10.5281/zenodo.18778766},
  note={Paper 4 of this series}
}

@article{baranov2026convergence,
  title={Sleep-Wake Memory Convergence in Weight-Edited Language Models},
  author={Baranov, Vladimir},
  journal={Zenodo},
  year={2026},
  doi={10.5281/zenodo.18778768},
  note={Paper 5 of this series}
}
