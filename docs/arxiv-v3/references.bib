% ═══════════════════════════════════════════════════════════════
% References for "Per-Fact Staged Consolidation" paper (v3)
% ═══════════════════════════════════════════════════════════════

% ── Knowledge editing: ROME, MEMIT ──

@inproceedings{meng2022rome,
  title={Locating and Editing Factual Associations in {GPT}},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@inproceedings{meng2023memit,
  title={Mass-Editing Memory in a Transformer},
  author={Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex J. and Belinkov, Yonatan and Bau, David},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@article{mitchell2022fast,
  title={Fast Model Editing at Scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Finn, Chelsea and Manning, Christopher D.},
  journal={arXiv preprint arXiv:2110.11309},
  year={2022}
}

@article{yao2023editing,
  title={Editing Large Language Models: Problems, Methods, and Opportunities},
  author={Yao, Yunzhi and Wang, Peng and Tian, Bozhong and Cheng, Siyuan and Li, Zhoubo and Deng, Shumin and Chen, Huajun and Zhang, Ningyu},
  journal={arXiv preprint arXiv:2305.13172},
  year={2023}
}

% ── LoRA and parameter-efficient fine-tuning ──

@inproceedings{hu2022lora,
  title={{LoRA}: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{dettmers2023qlora,
  title={{QLoRA}: Efficient Finetuning of Quantized {LLMs}},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@inproceedings{wang2023olora,
  title={{O-LoRA}: Orthogonal Subspace Learning for Language Model Continual Learning},
  author={Wang, Xiao and Chen, Tianze and Ge, Qiming and Xia, Han and Bao, Rong and Zheng, Rui and Zhang, Qi and Gui, Tao and Huang, Xuanjing},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={10658--10671},
  year={2023}
}

@inproceedings{liang2024inflora,
  title={{InfLoRA}: Interference-Free Low-Rank Adaptation for Continual Learning},
  author={Liang, Yan-Shuo and Li, Wu-Jun},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={23638--23647},
  year={2024}
}

% ── CLS theory and neuroscience ──

@article{mcclelland1995why,
  title={Why There Are Complementary Learning Systems in the Hippocampus and Neocortex: Insights from the Successes and Failures of Connectionist Models of Learning and Memory},
  author={McClelland, James L. and McNaughton, Bruce L. and O'Reilly, Randall C.},
  journal={Psychological Review},
  volume={102},
  number={3},
  pages={419--457},
  year={1995}
}

@article{kumaran2016what,
  title={What Learning Systems do Intelligent Agents Need? {Complementary Learning Systems} Theory Updated},
  author={Kumaran, Dharshan and Hassabis, Demis and McClelland, James L.},
  journal={Trends in Cognitive Sciences},
  volume={20},
  number={7},
  pages={512--534},
  year={2016}
}

@article{diekelmann2010memory,
  title={The Memory Function of Sleep},
  author={Diekelmann, Susanne and Born, Jan},
  journal={Nature Reviews Neuroscience},
  volume={11},
  number={2},
  pages={114--126},
  year={2010}
}

@article{rasch2013sleep,
  title={About Sleep's Role in Memory},
  author={Rasch, Bj{\"o}rn and Born, Jan},
  journal={Physiological Reviews},
  volume={93},
  number={2},
  pages={681--766},
  year={2013}
}

% ── Continual learning ──

@article{luo2023empirical,
  title={An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning},
  author={Luo, Yun and Yang, Zhen and Meng, Fandong and Li, Yafu and Zhou, Jie and Zhang, Yue},
  journal={arXiv preprint arXiv:2308.08747},
  year={2023}
}

@inproceedings{li2024revisiting,
  title={Revisiting Catastrophic Forgetting in Large Language Model Tuning},
  author={Li, Hongyu and Ding, Liang and Fang, Meng and Tao, Dacheng},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  year={2024}
}

@inproceedings{zheng2025spurious,
  title={Spurious Forgetting in Continual Learning of Language Models},
  author={Zheng, Jianheng and Cai, Xin and Qiu, Siheng and Ma, Qianli},
  booktitle={International Conference on Learning Representations},
  year={2025}
}

% ── RAG and memory-augmented models ──

@inproceedings{lewis2020retrieval,
  title={Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

% ── MLX framework ──

@misc{hannun2023mlx,
  title={{MLX}: Efficient and Flexible Machine Learning on {Apple} Silicon},
  author={Hannun, Awni and Digani, Jagrit and Katharopoulos, Angelos and Collobert, Ronan},
  year={2023},
  howpublished={Apple Machine Learning Research. \url{https://github.com/ml-explore/mlx}}
}

% ── Matrix algebra ──

@article{woodbury1950inverting,
  title={Inverting Modified Matrices},
  author={Woodbury, Max A.},
  journal={Memorandum Report},
  volume={42},
  pages={336},
  year={1950},
  publisher={Statistical Research Group, Princeton University}
}

% ── Our prior papers ──

@article{baranov2026sleepwake,
  title={Sleep-Wake Consolidation for Lifelong Conversational Memory in Local Language Models},
  author={Baranov, Vladimir},
  journal={arXiv preprint},
  year={2026},
  note={v1 of this work}
}

@article{baranov2026dualsystem,
  title={Dual-System Memory Consolidation for Lifelong Learning in Language Models: Combining Direct Weight Editing with Sleep-Wake Training},
  author={Baranov, Vladimir},
  journal={arXiv preprint},
  year={2026},
  note={v2 of this work}
}
