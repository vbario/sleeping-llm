% ═══════════════════════════════════════════════════════════════
% References for "Sleep-Wake Memory Convergence in Weight-Edited
% Language Models" (v5)
% ═══════════════════════════════════════════════════════════════

% ── Knowledge editing: ROME, MEMIT ──

@inproceedings{meng2022rome,
  title={Locating and Editing Factual Associations in {GPT}},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@inproceedings{meng2023memit,
  title={Mass-Editing Memory in a Transformer},
  author={Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex J. and Belinkov, Yonatan and Bau, David},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@article{mitchell2022fast,
  title={Fast Model Editing at Scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Finn, Chelsea and Manning, Christopher D.},
  journal={arXiv preprint arXiv:2110.11309},
  year={2022}
}

@article{yao2023editing,
  title={Editing Large Language Models: Problems, Methods, and Opportunities},
  author={Yao, Yunzhi and Wang, Peng and Tian, Bozhong and Cheng, Siyuan and Li, Zhoubo and Deng, Shumin and Chen, Huajun and Zhang, Ningyu},
  journal={arXiv preprint arXiv:2305.13172},
  year={2023}
}

% ── CLS theory, neuroscience, and sleep ──

@article{mcclelland1995why,
  title={Why There Are Complementary Learning Systems in the Hippocampus and Neocortex: Insights from the Successes and Failures of Connectionist Models of Learning and Memory},
  author={McClelland, James L. and McNaughton, Bruce L. and O'Reilly, Randall C.},
  journal={Psychological Review},
  volume={102},
  number={3},
  pages={419--457},
  year={1995}
}

@article{kumaran2016what,
  title={What Learning Systems do Intelligent Agents Need? {Complementary Learning Systems} Theory Updated},
  author={Kumaran, Dharshan and Hassabis, Demis and McClelland, James L.},
  journal={Trends in Cognitive Sciences},
  volume={20},
  number={7},
  pages={512--534},
  year={2016}
}

@article{diekelmann2010memory,
  title={The Memory Function of Sleep},
  author={Diekelmann, Susanne and Born, Jan},
  journal={Nature Reviews Neuroscience},
  volume={11},
  number={2},
  pages={114--126},
  year={2010}
}

@article{rasch2013sleep,
  title={About Sleep's Role in Memory},
  author={Rasch, Bj{\"o}rn and Born, Jan},
  journal={Physiological Reviews},
  volume={93},
  number={2},
  pages={681--766},
  year={2013}
}

@article{walker2004sleep,
  title={Sleep-Dependent Learning and Memory Consolidation},
  author={Walker, Matthew P. and Stickgold, Robert},
  journal={Neuron},
  volume={44},
  number={1},
  pages={121--133},
  year={2004}
}

@article{tononi2014sleep,
  title={Sleep and the Price of Plasticity: From Synaptic and Cellular Homeostasis to Memory Consolidation and Integration},
  author={Tononi, Giulio and Cirelli, Chiara},
  journal={Neuron},
  volume={81},
  number={1},
  pages={12--34},
  year={2014}
}

% ── Continual learning ──

@article{luo2023empirical,
  title={An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning},
  author={Luo, Yun and Yang, Zhen and Meng, Fandong and Li, Yafu and Zhou, Jie and Zhang, Yue},
  journal={arXiv preprint arXiv:2308.08747},
  year={2023}
}

@inproceedings{li2024revisiting,
  title={Revisiting Catastrophic Forgetting in Large Language Model Tuning},
  author={Li, Hongyu and Ding, Liang and Fang, Meng and Tao, Dacheng},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  year={2024}
}

@article{kirkpatrick2017overcoming,
  title={Overcoming Catastrophic Forgetting in Neural Networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Blundell, Charles and Wierstra, Daan and Hadsell, Raia},
  journal={Proceedings of the National Academy of Sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017}
}

@inproceedings{mallya2018packnet,
  title={{PackNet}: Adding Multiple Tasks to a Single Network by Iterative Pruning},
  author={Mallya, Arun and Lazebnik, Svetlana},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7765--7773},
  year={2018}
}

@article{robins1995catastrophic,
  title={Catastrophic Forgetting, Rehearsal and Pseudorehearsal},
  author={Robins, Anthony},
  journal={Connection Science},
  volume={7},
  number={2},
  pages={123--146},
  year={1995}
}

% ── Parameter-efficient fine-tuning (for comparison) ──

@inproceedings{hu2022lora,
  title={{LoRA}: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{dettmers2023qlora,
  title={{QLoRA}: Efficient Finetuning of Quantized {LLMs}},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

% ── CLS-inspired LLM/NN systems ──

@inproceedings{das2024larimar,
  title={Larimar: Large Language Models with Episodic Memory Control},
  author={Das, Payel and Chaudhury, Subhajit and Nelson, Elliot and Melnyk, Igor and Swaminathan, Sarath and Dai, Sihui and Lozano, Aur{\'e}lie and Banerjee, Georgios and Ghosh, Soham and Palatucci, Mark},
  booktitle={International Conference on Machine Learning},
  year={2024}
}

@article{sorrenti2024wscl,
  title={Wake-Sleep Consolidated Learning},
  author={Sorrenti, Amelia and Bellitto, Giovanni and Proietto Salanitri, Federica and Pennisi, Matteo and Palazzo, Simone and Spampinato, Concetto},
  journal={arXiv preprint arXiv:2401.08623},
  year={2024}
}

@article{behrouz2025sleep,
  title={Language Models Need Sleep: Learning to Self Modify and Consolidate Memories},
  author={Behrouz, Ali and Hashemi, Farnoosh and Mirrokni, Vahab},
  journal={arXiv preprint},
  year={2025}
}

% ── RAG ──

@inproceedings{lewis2020retrieval,
  title={Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

% ── Matrix algebra ──

@article{woodbury1950inverting,
  title={Inverting Modified Matrices},
  author={Woodbury, Max A.},
  journal={Memorandum Report},
  volume={42},
  pages={336},
  year={1950},
  publisher={Statistical Research Group, Princeton University}
}

% ── Our prior papers ──

@article{baranov2026sleepwake,
  title={Sleep-Wake Consolidation for Lifelong Conversational Memory in Local Language Models},
  author={Baranov, Vladimir},
  journal={arXiv preprint},
  year={2026},
  note={v1 of this work}
}

@article{baranov2026dualsystem,
  title={Dual-System Memory Consolidation for Lifelong Learning in Language Models: Combining Direct Weight Editing with Sleep-Wake Training},
  author={Baranov, Vladimir},
  journal={arXiv preprint},
  year={2026},
  note={v2 of this work}
}

@article{baranov2026perfact,
  title={Per-Fact Staged Consolidation for Lifelong Learning in Language Models: From Bulk Training to Granular Memory Management},
  author={Baranov, Vladimir},
  journal={arXiv preprint},
  year={2026},
  note={v3 of this work}
}

@article{baranov2026twophase,
  title={Sleeping {LLM}: Two-Phase Memory Consolidation for Lifelong Learning from {3B} to {70B} Parameters},
  author={Baranov, Vladimir},
  journal={arXiv preprint},
  year={2026},
  note={v4 of this work}
}
