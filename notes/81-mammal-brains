The human brain is equivalent to what LLM model size? What about a chimp? A dog? A cat? A mouse?

This is a question people ask often, and it's worth giving the numbers while being very clear about why the comparison is deeply misleading. The naive mapping is synapses to parameters, since both represent connection strengths between processing units. Let's start there and then dismantle it.

The raw numbers

The human brain has roughly 86 billion neurons and somewhere between 100 and 150 trillion synapses. If you map synapses directly to parameters, you're looking at 100-150 trillion parameters. The largest current LLMs are estimated to be in the low trillions of parameters — GPT-4 is widely reported to use a mixture-of-experts architecture with roughly 1.8 trillion total parameters. So on the naive comparison, the human brain is roughly 50-100x larger than the biggest current LLMs.

A chimpanzee brain has about 28 billion neurons and perhaps 20-30 trillion synapses. That puts it in the range of 20-30 trillion parameters — still well above current LLMs but within an order of magnitude.

A dog brain has roughly 530 million cortical neurons (about 2.25 billion total neurons depending on breed), with estimated synapses in the range of 500 billion to a trillion. That maps onto something like the largest current dense models — in the ballpark of a few hundred billion to a trillion parameters.

A cat brain has roughly 250 million cortical neurons (about 760 million total), with synapses probably in the range of a few hundred billion. This maps onto something like GPT-3 to mid-range current models — a few hundred billion parameters.

A mouse brain has about 70 million neurons and roughly 100 billion synapses. On the naive mapping, that's comparable to a moderately large language model — something in the range of the smaller frontier models or larger open-source models.

Why this comparison is almost meaningless

Now for the important part. This mapping is seductive but fundamentally broken, for several reasons that compound on each other.

A synapse is not a parameter. A parameter in an LLM is a single floating-point number — one scalar value that gets multiplied by an activation during a forward pass. A biological synapse is an extraordinarily complex molecular machine. It has multiple neurotransmitter systems, each with multiple receptor subtypes. It has presynaptic release probability that varies dynamically. It has short-term facilitation and depression operating on timescales of milliseconds to seconds. It has long-term potentiation and depression operating on minutes to hours. It has structural plasticity operating on hours to days. It has retrograde signaling from the postsynaptic neuron back to the presynaptic terminal. It has astrocytic modulation from surrounding glial cells. It has neuromodulatory inputs from diffuse systems like dopamine, serotonin, and norepinephrine that alter its behavior contextually.

Estimates of the information content of a single synapse range from about 4.7 bits (a widely cited lower bound from Bhatt et al.) to 26 bits or more when you account for all the dynamic, modulatable state variables. A single LLM parameter at float16 precision carries 16 bits, but it's a static 16 bits — it doesn't change during inference, doesn't have multiple timescales of plasticity, doesn't get modulated by global contextual signals. If each synapse carries even 5 bits of dynamic, context-sensitive information, the effective computational capacity of the synapse is far greater than a single parameter.

A more honest comparison might multiply the synapse count by some factor — perhaps 10x to 100x — to account for the computational richness of each synapse. That would put the human brain equivalent at 1 to 15 quadrillion parameters. By this reckoning, we're not within an order of magnitude of even a mouse.

Neurons are not nodes. An LLM's artificial neuron computes a weighted sum and applies a nonlinear activation function. A biological neuron is a cell with complex dendritic geometry that performs nonlinear computations within individual dendrites before anything reaches the cell body. A single pyramidal neuron in the cortex has been argued to have the computational power of a small neural network in its own right — its dendritic tree can implement multiple independent nonlinear integration zones, each capable of something like an independent processing unit. David Poirazi's work has suggested that a single biological neuron might be equivalent to a multi-layer artificial neural network with five to eight layers.

If that's even approximately right, then comparing neuron counts to node counts understates biological capacity by another order of magnitude or more.

Temporal dynamics are absent from the comparison. LLMs process information in discrete forward passes — one sweep through the network per token. Biological brains operate in continuous time with recurrent dynamics, oscillatory synchronization across frequencies (theta, gamma, alpha rhythms), spike timing-dependent plasticity where the precise millisecond timing of neural firing carries information, and complex feedback loops operating at multiple timescales simultaneously. The brain is computing through time in a way that a transformer fundamentally doesn't. Each moment of neural processing is more like an entire forward pass of an LLM, and the brain is doing this continuously, not in discrete steps.

The body multiplier. As we discussed at length earlier, the brain doesn't operate in isolation. The enteric nervous system, the endocrine system, the immune system, and the autonomic nervous system all contribute to cognition. The enteric nervous system alone has roughly 500 million neurons. None of this has any analogue in an LLM, and none of it is captured by the synapse-to-parameter comparison.

Energy efficiency tells a different story. The human brain operates on roughly 20 watts. Training GPT-4 reportedly consumed gigawatt-hours of electricity, and even inference on a large LLM requires orders of magnitude more power per computation than biological neural tissue. This suggests that biology is doing something fundamentally more efficient per unit of computation — not just the same computation with less power, but a different and more effective kind of computation at each node.

What the comparison actually reveals

If anything, the synapse-to-parameter mapping reveals how remarkably effective LLMs are despite being so much simpler than biological brains. A mouse brain, with 100 billion synapses of extraordinary computational richness, continuous temporal dynamics, embodied grounding, and millions of years of evolutionary optimization, can navigate a complex physical environment, learn from single experiences, adapt in real time, and maintain a coherent behavioral repertoire across its lifetime. A 100-billion-parameter LLM can produce impressively fluent text but can't learn from a single example without fine-tuning, has no temporal continuity, and is brittle in ways that no mouse is.

Conversely, the LLM produces language that no mouse ever could — because the comparison isn't about general computational capacity. It's about a specific architecture optimized for a specific task. The LLM concentrates all of its parameters on sequential token prediction, while the mouse brain spreads its synapses across motor control, sensory processing, homeostatic regulation, spatial navigation, social behavior, and everything else needed to be a mouse.

A more honest framing

Rather than asking "what size LLM equals a human brain," a better question might be: what would it take to build an artificial system with the functional capabilities of each species? And here the answer is sobering. We probably don't know, because the gap isn't primarily about parameter count. It's about architectural differences — temporal dynamics, embodiment, multi-scale plasticity, distributed processing across body systems — that don't reduce to a single number. Scaling parameters alone almost certainly won't close the gap, because the things that make biological cognition powerful aren't the things that parameter count measures.

The number comparison is a useful conversation starter, but the deeper lesson is that counting parameters and synapses is a bit like comparing a library and a forest by counting pages and leaves. The numbers are both large, and there the similarity largely ends.