⏺ MEMIT + LoRA Sleep Cycle

  During the day (waking):

  The user shares a fact. You run MEMIT to edit the feedforward weights at layers 15-17 (or wherever the model stores factual associations).
  You also log the edit: {fact: "Dog's name is Biscuit", layer: 16, edit_vector: [...]}. The model can immediately recall the fact.

  During sleep (consolidation):

  1. Extract training data as usual — but now you have an advantage. The MEMIT-edited model already knows the facts, so when the curator asks
  it to extract Q&A pairs from the conversation, it's working with a model that can actually verify them. This should improve extraction
  quality.
  2. Train LoRA on the facts — same as now. The training data includes all the facts that were MEMIT-edited during the day, formatted as Q&A
  pairs. The LoRA adapter learns to reproduce these answers.
  3. Validate — same as now. Test that the LoRA-fused model recalls facts correctly.
  4. Revert MEMIT edits — this is the key step. You saved the original feedforward weights before each MEMIT edit (or you keep a clean
  checkpoint). After LoRA fusion passes validation, restore the feedforward layers to their clean state. The facts now live in the LoRA
  weights instead.
  5. Verify after revert — test recall again. If the LoRA adapter successfully absorbed the facts, recall should hold. If some facts drop, you
   can either re-apply those specific MEMIT edits or flag them for reinforcement in the next sleep cycle.

  The result: Feedforward weights stay clean — no accumulation of hundreds of rank-one edits degrading the model over time. LoRA carries the
  long-term memory. MEMIT is a scratch pad that gets erased each night.

  The clean checkpoint management is straightforward. You already keep models/base and models/current. You'd add models/clean — the
  post-LoRA-fusion state before any MEMIT edits. Each morning starts from clean + LoRA. Each day accumulates MEMIT edits on top. Each night
  consolidates into LoRA and resets to clean.
