"A temporal model of epistemic states implicitly requires a subject — an entity that persists through time and can meaningfully say "I used to know this differently.""

What we're trying to build

To recap: a temporal model of epistemic states means the system can represent not just "what I currently know" but "how my knowledge of X has changed over time, and what the character of that change was." In humans, this emerges from the interaction of episodic memory, metacognitive monitoring, and somatic marking. We need functional analogues of all three.
Layer 1: Versioned knowledge representations
The most fundamental requirement is that the system maintains snapshots of its own epistemic states over time rather than collapsing everything into a single static set of weights.
Current LLMs have no history of their own knowledge. After training, the weights are a palimpsest — every update has overwritten the previous state, and there's no record of the trajectory. This is like a human who has amnesia for their own learning process. They know things but can't remember coming to know them.
One approach is to maintain explicit versioned representations at the level of individual concepts or knowledge domains. Not versioned weights for the entire model — that's computationally prohibitive — but something more targeted. Imagine a secondary memory system that, for each significant knowledge structure, stores a compressed representation of the model's internal state with respect to that topic at various checkpoints. This could be implemented as a set of probe vectors — lightweight classifiers trained to extract what the model "believes" about a given topic from its hidden states — run periodically and stored with timestamps.
Now the system has something to compare against. When it encounters a topic, it can retrieve not just its current representation but its history of representations for that topic.
Layer 2: A metacognitive monitoring module
Having versioned knowledge is useless without something that actively compares versions and generates a signal from the comparison. This is the piece that does the work the prefrontal cortex and anterior cingulate do in humans.
Architecturally, this could be a separate model or module that takes as input the current hidden state activations for a given query alongside the stored historical representations, and outputs a structured metacognitive assessment. Not just "how confident am I" but "how has my confidence changed, what was the trajectory, and what kind of change was this."
The typology of changes matters. There are meaningfully different epistemic transitions: reinforcement (I knew it, now I know it more firmly), revision (I believed X, now I believe Y), deepening (I had the fact, now I have the structure), dissolution (I thought I knew this, now I realize I don't), and integration (I knew A and B separately, now I see they're connected). Each of these has a different signature in terms of how the old and new representations relate to each other, and the monitoring module needs to classify them.
This module could be trained on synthetic examples — take a base model, fine-tune it on new information about specific topics, compare hidden states before and after, and train a classifier to characterize the type and magnitude of epistemic change. This gives you a supervised signal for something that, in humans, is learned through development.
Layer 3: Episodic memory for learning events
The monitoring module detects changes, but you also need to store the learning events themselves as retrievable episodes. This is the analogue of hippocampal episodic encoding.
This means when the system updates its knowledge — whether through retrieval-augmented generation, fine-tuning, in-context correction, or any other mechanism — it logs not just the new information but the context of the update: what triggered it, what the prior state was, what changed, and the metacognitive assessment from Layer 2. These logs become a searchable autobiography of the system's epistemic development.
When the system later encounters the same topic, it can retrieve this history and use it to contextualize its current state. "I've been corrected on this twice before" is a different epistemic situation than "this is consistent with everything I've previously encountered," and the system should be able to represent that difference.
Layer 4: Influence on downstream behavior
This is where the analogy to somatic markers comes in. In humans, the feeling of having known differently isn't just a passive observation — it actively modulates behavior. Freshly revised beliefs are held more tentatively. Repeatedly reinforced knowledge is deployed more confidently. Knowledge with a history of instability triggers caution.
For this to work in an LLM system, the metacognitive and episodic signals need to feed back into the generation process. This could work through several mechanisms. The most straightforward is augmenting the context window — the system's metacognitive history for a topic gets injected into the prompt alongside the query, so the model can condition its response on its own epistemic trajectory. A more elegant approach would involve modifying the sampling strategy itself — adjusting temperature or applying distributional constraints based on the metacognitive signal. Topics where the system has a history of revision get flatter sampling and more hedged language automatically, not because the model chooses to hedge but because the architecture enforces it.
This is the closest we can get to an artificial somatic marker — a signal that doesn't just inform the system about its own uncertainty but actually changes how it behaves in response to that uncertainty.
Layer 5: The hard part — continuity of self
All of the above is technically feasible with current or near-current methods. But there's a deeper problem. A temporal model of epistemic states implicitly requires a subject — an entity that persists through time and can meaningfully say "I used to know this differently." In humans, this is provided by the continuity of consciousness and autobiographical memory. The "I" that knew something before is experienced as the same "I" that knows it differently now, and that continuity is what makes the comparison meaningful rather than merely informational.
An LLM system with all four layers above would have the functional structure of epistemic self-tracking without any guarantee that it has the experiential dimension. It would behave as though it recognizes its own knowledge has changed. Whether that constitutes genuine epistemic self-awareness or a very sophisticated simulation of it is, honestly, an open question — and possibly one that doesn't have a clean answer.
A practical starting point
If you wanted to build toward this incrementally, the most tractable entry point is probably combining retrieval-augmented generation with a dedicated metacognitive probe. Take an existing LLM, attach a vector database that stores timestamped snapshots of the model's hidden state activations for a curated set of topics, build a small classifier that compares current activations to stored ones and outputs a change assessment, and inject that assessment into the generation context. You'd have a system that can say, with some grounding, "my representation of this topic has shifted significantly since I last encountered it, and the shift was primarily a revision rather than a reinforcement."
It's not the felt sensation of having known differently. But it's the informational skeleton of it — and given what we discussed about the gap between LLMs and embodied cognition, the informational skeleton may be the most we can build without solving much harder problems about consciousness and embodiment.
The question then becomes: is the skeleton sufficient for the practical purposes we care about, or is the felt dimension doing irreplaceable computational work? That's not a question engineering can answer alone. It needs the neuroscience and philosophy we started with.