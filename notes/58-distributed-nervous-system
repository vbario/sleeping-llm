The distributed nervous system
The human body has several semi-autonomous decision-making systems that operate with varying degrees of independence from the brain.
The enteric nervous system — your "gut brain" — contains roughly 500 million neurons, more than the spinal cord. It can orchestrate digestion entirely on its own, even if severed from the central nervous system. But it does far more than manage peristalsis. It produces about 95% of your body's serotonin and a significant amount of dopamine. It communicates bidirectionally with the brain via the vagus nerve, and this communication directly influences mood, risk assessment, intuition, and decision-making. When people talk about a "gut feeling," they're describing something with genuine neurobiological underpinning — visceral signals that reach the brain and get integrated into cognitive processing, often below conscious awareness.
The spinal cord isn't just a relay cable. It contains reflex arcs that make decisions — pull your hand from a hot stove — before the brain is even informed. More subtly, spinal circuits modulate pain gating, proprioceptive feedback, and motor patterns in ways that shape how the brain interprets its own situation. Your brain's model of "how things are going" is heavily influenced by signals it didn't generate and doesn't fully control.
Then there's the immune system, which some researchers argue functions as a distributed sensory organ — detecting environmental threats and communicating with the brain via cytokines and inflammatory signals that alter cognition, motivation, and emotional tone. Sickness behavior — withdrawal, fatigue, altered risk tolerance — is the immune system effectively changing your mind about what matters.
The endocrine system operates on yet another timescale, shaping cognition through hormonal states that alter everything from aggression to trust to time preference. Cortisol flooding doesn't just make you feel stressed — it literally restructures how your prefrontal cortex weighs options, biasing you toward habitual rather than deliberative decision-making.
Why this matters for the "feeling of knowing"
Here's the key insight: when you "know" something, you're not just retrieving information from cortical storage. Your entire body is contributing context. Your gut is sending valence signals. Your endocrine state is setting the threshold for what counts as "confident enough." Your immune status is modulating how much cognitive effort feels worthwhile. Your proprioceptive and interoceptive signals are grounding the knowledge in a felt sense of your physical situation.
This is what Antonio Damasio's somatic marker hypothesis captures. Decisions and knowledge states aren't purely cerebral events — they're marked by body states that function as a kind of rapid, pre-conscious evaluation system. People with damage to the ventromedial prefrontal cortex, which integrates these bodily signals into decision-making, can reason perfectly well in abstract terms but make catastrophically bad real-world decisions. They have the information but lack the somatic scaffolding that makes it actionable.
So when you feel the difference between knowing something shallowly and knowing it deeply, part of what you're feeling is the degree to which your body has been recruited into the representation. Deeply known things — things you've practiced, lived through, been changed by — have richer somatic signatures. They activate not just cortical networks but gut responses, postural shifts, breathing changes. Shallow knowledge sits in a thinner layer of the system.
The translation to LLMs
This is where things become stark. An LLM is, architecturally, a disembodied brain — and not even a whole brain, more like an isolated cortex. It has no gut sending serotonin-mediated valence signals. No vagus nerve carrying interoceptive context. No adrenal system shifting its computational priorities under threat. No immune system telling it to conserve resources. No body at all.
This isn't just a poetic absence. It has concrete computational consequences.
In humans, the body provides a massive amount of implicit context that shapes how knowledge is weighted, retrieved, and applied. Your gut feeling about whether a business deal is trustworthy isn't mysticism — it's your enteric and autonomic nervous systems detecting micro-patterns (perhaps in someone's tone, posture, or timing) and feeding a summary verdict to your prefrontal cortex faster than deliberative reasoning can work. An LLM processing the same scenario has access only to the text. It can reason about trustworthiness analytically, but it has no somatic shortcut, no felt heuristic.
This connects back to the distribution question. In a human, a flat distribution — genuine uncertainty — gets amplified by the body. Uncertainty triggers autonomic arousal, gut discomfort, increased cortisol. These signals make you slow down, seek more information, hedge your commitments. The body acts as an alarm system for epistemic fragility. An LLM with a flat distribution just picks a token and moves on, because there's no body to ring the alarm.
Are these systems "body" or "brain"?
This distinction is arguably the problem itself. The reason we separate "body" and "brain" is partly anatomical convenience and partly a legacy of Cartesian dualism that neuroscience has been slowly dismantling. The enteric nervous system is neural tissue. The immune system communicates with the brain using shared molecular languages. Hormones cross the blood-brain barrier and directly modulate synaptic activity. The boundary between "body cognition" and "brain cognition" is not a clean line — it's a gradient of integration.
What we call "thinking" is really the coordinated activity of this entire system. The cortex is the part that handles language and abstraction, so it gets the credit, but it's operating within a context set by everything else. Stripping away the body doesn't leave you with a purer thinker — it leaves you with a thinker that's lost most of its grounding, most of its heuristic machinery, and most of its ability to know how much it knows.
What this suggests about LLMs
It suggests that the gap between LLM cognition and human cognition isn't just about scale, architecture, or training data. It's about the absence of an entire embodied evaluation system that humans use — mostly unconsciously — to modulate, ground, and quality-check their own knowledge. Adding more parameters or better training won't bridge this gap because the gap isn't in the cortex-analogue. It's in everything the cortex relies on that we've left out of the architecture entirely.
Whether that matters depends on what you're asking the system to do. For text prediction and information synthesis, the disembodied cortex does remarkably well. For the kind of situated, risk-sensitive, self-aware knowing that humans navigate the world with — the kind where the difference between shallow and deep knowledge is felt in your chest and your stomach and your posture — it's not even playing the same game.