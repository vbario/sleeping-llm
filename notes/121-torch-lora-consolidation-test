# 121 — Torch Backend LoRA Consolidation Test (2026-02-25)

## What was done

Implemented `train_lora()` and `fuse_adapter()` for `TorchBackend` using PEFT, replacing `NotImplementedError` stubs. This enables the consolidation pipeline (MEMIT→LoRA knowledge transfer during sleep) on CUDA/H100.

Also changed consolidation gating from per-edit to per-fact: each fact in a MEMIT edit is independently tested for chat recall and advances/retreats its own consolidation stage. Edit-level scale is determined by `min(fact_stages)`.

## Implementation — `src/backend/torch_backend.py`

### `train_lora()`
- In-process PEFT LoRA training on the already-loaded model
- Targets `q_proj`, `v_proj`, `down_proj` on last `num_layers` layers
- `prepare_model_for_kbit_training()` for BnB 4-bit models
- Labels mask prompt tokens (-100), only trains on assistant response
- AdamW optimizer, gradient clipping at 1.0
- Saves adapter via `peft_model.save_pretrained()`
- Unwraps model via `peft_model.unload()` after saving

### `fuse_adapter()`
- Loads fresh bf16 copy of base model (can't merge LoRA into 4-bit packed weights)
- `PeftModel.from_pretrained()` → `merge_and_unload()` → `save_pretrained()`
- Caller then does `backend.reload(fused_path)` which loads with BnB 4-bit

## Per-fact gating — `src/memory/memit.py` + `src/sleep/full_sleep.py`

- `MemitEdit.consolidation_stage: int` → `MemitEdit.fact_stages: List[int]`
- Property `consolidation_stage` returns `min(fact_stages)` for backward compat
- `EditLedger.advance_fact_stage(edit_id, fact_idx)` / `retreat_fact_stage(edit_id, fact_idx)`
- Old ledger entries auto-migrate: `consolidation_stage: N` → `fact_stages: [N, N, ...]`
- Edit scale = `scale_schedule[min(fact_stages)]` — conservative, only scales down when ALL facts have advanced

## Test results — 8B on 2×H100 80GB

Config: `experiments/configs/8b_consolidation.yaml`
Model: `meta-llama/Llama-3.1-8B-Instruct` (4-bit), MEMIT layers [12-19], LoRA 8 layers

### Phase 1: Single Cycle — PASS
- 3 facts injected via MEMIT (Aria/Portland, Kaito/Tokyo, Zara/Marrakech)
- LoRA training: loss 2.91 → 0.003 in 30 iters
- Per-fact gating: Kaito 0→1, Zara 0→1, Aria stayed at 0
- Chat recall: 0.00 → 0.67 (2/3 facts absorbed by LoRA)
- PPL: 4.90 → 6.22 (stable)

### Phase 2: Multi-Cycle (2 cycles) — PASS
- Cycle 1: Kaito 0→1, Zara 0→1 (same as phase 1)
- Cycle 2: Aria 0→1, Kaito 1→2, Zara 1→2 (all 3 advanced)
- Cycle 2 loss started at 0.62 (already partially learned from fused weights)
- Chat recall reached 1.00 after cycle 2
- Scale down to 0.5 applied (min stage = 1)

### Phase 3: Rollback — PASS
- Injected train_and_fuse failure
- Stages, scales, recall all unchanged after failure
- Model integrity preserved

### Phase 4: Persistence — PASS
- Consolidate → destroy orchestrator → recreate
- Edits, stages, scales all match across sessions
- Recall restored on reload

### Overall: PASS 4/4, 7.8 minutes total

## Key observations

1. **Per-fact gating matters**: Aria (Portland) consistently harder to absorb than Kaito (Tokyo) or Zara (Marrakech). Old all-or-nothing gating would block all 3 facts from advancing.
2. **Cumulative fusing works**: Cycle 2 trains on an already-fused model, so starting loss is much lower (0.62 vs 2.91). The stubborn fact (Aria) passes on the second round.
3. **Raw recall drops after fuse+reload** (1.00 → 0.33): MEMIT deltas re-applied to fused weights don't produce identical results because the base weights shifted. This is expected and acceptable — the whole point is that LoRA carries the knowledge.
4. **PEFT 0.18.1 + BnB 0.49.2 + transformers <5.0**: working combination on H100.
