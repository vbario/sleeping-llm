# Sleeping LLM Configuration

model:
  # Backend: "mlx" (Apple Silicon) or "torch" (CUDA GPU)
  backend: "mlx"
  # Path to MLX model (local or HuggingFace hub ID)
  path: "mlx-community/Llama-3.2-3B-Instruct-4bit"
  # Max tokens for generation
  max_tokens: 512
  # Sampling temperature
  temperature: 0.7
  # Top-p sampling
  top_p: 0.9
  # Repetition penalty
  repetition_penalty: 1.1

context:
  # Max context window size in tokens
  max_tokens: 4096
  # Trigger compaction when context reaches this fraction of max
  compaction_threshold: 0.8
  # System prompt for the model
  system_prompt: "You may recall things from previous conversations."

sleep:
  # Number of conversation turns before triggering light sleep
  light_sleep_turns: 5
  # Number of light sleep cycles before triggering deep sleep
  deep_sleep_interval: 5
  # Manual sleep command (user can type this to trigger sleep)
  manual_trigger: "/sleep"

  # Curation thresholds (0.0 - 1.0) — set to 0 to keep everything
  curation:
    min_novelty_score: 0.0
    min_importance_score: 0.0
    min_combined_score: 0.0

  # Hallucination firewall — verifies extracted Q&A pairs against source conversation
  firewall:
    # Minimum fraction of claims that must appear in conversation (0.0 - 1.0)
    min_grounding_score: 0.3
    # Use model to double-check borderline cases (slower but more accurate)
    use_model_verification: false

lora:
  # LoRA rank (higher = more capacity to learn)
  rank: 16
  # LoRA alpha (higher = stronger updates)
  alpha: 32
  # Target layers for LoRA
  layers: 8
  # Learning rate for light sleep
  light_learning_rate: 1.0e-4
  # Learning rate for deep sleep
  deep_learning_rate: 5.0e-5
  # Training epochs: each example seen N times per sleep
  light_epochs: 3
  # Deep sleep: more passes
  deep_epochs: 2
  # Batch size
  batch_size: 1

replay:
  # Max items in replay buffer
  max_items: 1000
  # Fraction of ACTIVE replay data to mix into each training run
  # Light sleep: low ratio so new facts dominate
  light_mix_ratio: 0.2
  # Deep sleep: higher ratio for consolidation
  deep_mix_ratio: 0.6
  # Decay factor for spaced repetition (lower = forget faster)
  # At 0.85: ~7 replays to halve priority, ~15 replays to reach floor
  decay_factor: 0.85
  # Items below this priority stop being trained on (but stay in buffer)
  min_priority: 0.05

validation:
  # If eval score drops below this fraction of baseline, block the merge
  # Set low for testing — accept more degradation to prioritize learning
  min_score_ratio: 0.5
  # Number of eval questions to run
  num_questions: 5

dreamer:
  # Number of synthetic examples to generate per deep sleep
  num_dreams: 10
  # Temperature for dream generation (higher = more creative)
  temperature: 0.9

paths:
  conversations: "data/conversations"
  training: "data/training"
  replay_buffer: "data/replay_buffer"
  core_identity: "data/core_identity"
  benchmarks: "data/benchmarks"
  base_model: "models/base"
  current_model: "models/current"
  checkpoints: "models/checkpoints"
  adapters: "adapters"
