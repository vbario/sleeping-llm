# Sleeping LLM Configuration

model:
  # Backend: "mlx" (Apple Silicon) or "torch" (CUDA GPU)
  backend: "mlx"
  # Path to MLX model (local or HuggingFace hub ID)
  path: "mlx-community/Llama-3.2-3B-Instruct-4bit"
  # Max tokens for generation
  max_tokens: 512
  # Sampling temperature
  temperature: 0.7
  # Top-p sampling
  top_p: 0.9
  # Repetition penalty
  repetition_penalty: 1.1

context:
  # Max context window size in tokens
  max_tokens: 4096
  # Trigger compaction when context reaches this fraction of max
  compaction_threshold: 0.8
  # System prompt for the model
  system_prompt: "You may recall things from previous conversations."

sleep:
  # Manual sleep command (user can type this to trigger sleep)
  manual_trigger: "/sleep"
  # Trigger mode: "health" (MEMIT-based pressure) or "turns" (original fixed counter)
  trigger_mode: "health"

  # Curation thresholds (0.0 - 1.0) — set to 0 to keep everything
  curation:
    min_novelty_score: 0.0
    min_importance_score: 0.0
    min_combined_score: 0.0

  # Hallucination firewall — verifies extracted Q&A pairs against source conversation
  firewall:
    # Minimum fraction of claims that must appear in conversation (0.0 - 1.0)
    min_grounding_score: 0.3
    # Use model to double-check borderline cases (slower but more accurate)
    use_model_verification: false

  # MEMIT maintenance during sleep
  maintenance:
    # Recall rate below which a fact is re-injected during sleep
    degraded_threshold: 0.5
    # Rollback if PPL increases more than this fraction
    max_ppl_increase: 0.15
    # Max facts to re-inject per sleep cycle
    max_refresh_per_cycle: 10
    # How many recent facts nap audits
    nap_audit_count: 5

memit:
  # Range of transformer layers to target for MEMIT edits
  # For Llama-3.2-3B (26 layers), middle layers store factual knowledge
  target_layers: [8, 9, 10, 11, 12, 13, 14, 15]
  # Regularization parameter (higher = more conservative edits)
  lambda_reg: 0.1
  # Maximum active MEMIT edits (individual facts) before requiring sleep
  max_active_edits: 50
  # Whether to enable MEMIT injection during wake
  enabled: true
  # MLP module to edit
  target_module: "down_proj"
  # Covariance estimation: use reference texts to estimate per-dimension activation
  # variance for proper MEMIT regularization (prevents multi-fact interference).
  # 0 = skip (use identity matrix), >0 = estimate from reference texts
  covariance_samples: 200
  # Residual scale factor (compensates for null-space constraints + regularization losses)
  residual_scale: 2.0
  # v* optimization parameters
  v_lr: 0.5
  v_steps: 30
  v_kl_factor: 0.0625

lora:
  enabled: true
  num_layers: 8
  learning_rate: 1.0e-4
  iters_per_fact: 10
  batch_size: 1

consolidation:
  enabled: true
  scale_schedule: [1.0, 0.5, 0.1, 0.0]

health:
  # Sleep pressure threshold to suggest a nap (0.0 - 1.0)
  nap_threshold: 0.4
  # Sleep pressure threshold to require full sleep (0.0 - 1.0)
  sleep_threshold: 0.8
  # Weights for pressure calculation
  edit_weight: 0.6
  time_weight: 0.3
  perplexity_weight: 0.1
  # Measure perplexity every N edits (0 = never)
  perplexity_check_interval: 10
  # Max wake duration in seconds before sleep is forced
  max_wake_seconds: 7200

nap:
  # Manual trigger command
  manual_trigger: "/nap"

validation:
  # If eval score drops below this fraction of baseline, block the merge
  # Set low for testing — accept more degradation to prioritize learning
  min_score_ratio: 0.5
  # Number of eval questions to run
  num_questions: 5

paths:
  conversations: "data/conversations"
  core_identity: "data/core_identity"
  benchmarks: "data/benchmarks"
  memit_data: "data/memit"
  memit_ledger: "data/memit/ledger.json"
  adapters: "data/adapters"
  fused_models: "models/fused"
